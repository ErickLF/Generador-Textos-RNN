{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generador de municipios\n",
    "### Erick Fernando López Fimbres.  \n",
    "#### erick.lopez.fimbres@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición\n",
    "\n",
    "Para la realizacion de este programa usaremos Nivel de caracter minimo y puedes consultar el archivo original [aquí](https://gist.github.com/karpathy/d4dee566867f8291f086). \n",
    "El cual nos ayudara a generar nombres de municipios caracter por caracter a partir de los ya existentes que hay en México.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "municipios = open('datos/municipios.txt', 'r').read() # should be simple plain text file\n",
    "caracteres = list(set(municipios))\n",
    "tam_datos, tam_caracteres = len(municipios), len(caracteres)\n",
    "print (\"Hay {} municipios y {} caracteres\".format(municipios,caracteres))\n",
    "caracter_a_indice = { ch:i for i,ch in enumerate(chars) }\n",
    "indice_a_caracter = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustes de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "capas_ocultas = 100 # tamaño de capas ocultas de neuronas\n",
    "num_pasos = 25 # numero de pasos para el RNN\n",
    "epsilon = 1e-1\n",
    "\n",
    "# parametros del modelo\n",
    "Wxh = np.random.randn(capas_ocultas, tam_caracteres)*0.01 # oculto a entrada\n",
    "Whh = np.random.randn(capas_ocultas, capas_ocultas)*0.01 # oculto a oculto\n",
    "Why = np.random.randn(tam_caracteres, capas_ocultas)*0.01 # oculto a la salida\n",
    "bh = np.zeros((capas_ocultas, 1)) # sesgo oculto\n",
    "by = np.zeros((tam_caracteres, 1)) # sesgo de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(entradas, objetivos, hprev):\n",
    "    \"\"\"\n",
    "    entradas: una lista de enteros\n",
    "    objetivos: una lista de enteros\n",
    "    hprev es un arreglo de  Hx1 del estado oculto inicial\n",
    "    regresa: la perdida, gradientes del modelo y el ultimo estado oculto\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1]= np.copy(hprev)\n",
    "    perdida = 0\n",
    "    # forward pass\n",
    "    for t in range(len(entradas)):\n",
    "        xs[t] = np.zeros((tam_caracteres,1)) # encode in 1-of-k representation\n",
    "        xs[t][entradas[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        perdida += -np.log(ps[t][objetivos[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(entradas))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[objetivos[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return perdida, dWxh, dWhh, dWhy, dbh, dby, hs[len(entradas)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 10 municipios de México: \n",
      "\n",
      "Abalá\n",
      "\n",
      "Abasolo\n",
      "\n",
      "Abejones\n",
      "\n",
      "Acacoyagua\n",
      "\n",
      "Acajete\n",
      "\n",
      "Acala\n",
      "\n",
      "Acámbaro\n",
      "\n",
      "Acambay de Ruíz Castañeda\n",
      "\n",
      "Acanceh\n",
      "\n",
      "Acapetahua\n",
      "\n",
      "Número de municipios en total:  2318\n"
     ]
    }
   ],
   "source": [
    "#primero leeremos los archivos de los municipios\n",
    "\"\"\"\n",
    "archivo=open('datos/municipios.txt')\n",
    "linea=archivo.readline()\n",
    "datos_limpios=list()\n",
    "while linea != \"\":\n",
    "    if linea not in datos_limpios:\n",
    "        datos_limpios.append(linea)    \n",
    "    linea=archivo.readline()\n",
    "archivo.close()\n",
    "\n",
    "print(\"Primeros 10 municipios de México: \\n\")\n",
    "for x in range(10):\n",
    "    print(datos_limpios[x])\n",
    "print(\"Número de municipios en total: \",len(datos_limpios))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo, ahora tenemos nuestros datos de municipios sin que esten repetidos, ahora necesitamos comenzan con las Redes LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import code\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    @staticmethod\n",
    "    def init(tam_entrada,tam_oculto,sesgo_inicial=3):\n",
    "        \"\"\"inicializa los parametros de LSTM (sesgos y pesos en una matriz)\"\"\"\n",
    "        #Inicializa la matriz de pesos\n",
    "        WLSTM = np.random.randn(tam_entrada + tam_oculto + 1, 4 * tam_oculto) / np.sqrt(tam_entrada + tam_oculto)\n",
    "        WLSTM[0,:] = 0 # initializamos en 0\n",
    "        if sesgo_inicial != 0:\n",
    "            #Las entradas de olvido en un principio tienen un sesgo negativo para alentarlas a que se apaguen\n",
    "            #la no linealidad son de media cero y desviación estándar uno\n",
    "            WLSTM[0,tam_oculto:2*tam_oculto] = sesgo_inicial\n",
    "        return WLSTM\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(X,WLSTM,c0=None,h0=None):\n",
    "        \"\"\"X debe de ser de la forma (n, b, tam_entrada), donde n = longitud de secuencia, b = tamaño del batch o lote\"\"\"\n",
    "        n,b,tam_entrada = X.shape\n",
    "        d = WLSTM.shape[1]/4 #tamaño oculto\n",
    "        if c0 is None: c0 = np.zeros((b,d))\n",
    "        if h0 is None: h0 = np.zeros((b,d))\n",
    "        #ejecutar el fordward de LSTM con X como entrada\n",
    "        \n",
    "        xmhms = WLSTM.shape[0] # x mas h mas sesgo\n",
    "        Hin = np.zeros((n, b, xmhms)) # entrada [1, xt, ht-1] para cada momento del LSTM\n",
    "        Hout = np.zeros((n, b, d)) # representacion oculta del LSTM\n",
    "        \n",
    "        EOSP = np.zeros((n, b, d * 4)) # entrada, olvido, salida, puerta(IFOG)\n",
    "        EOSPd = np.zeros((n, b, d * 4)) # despues de la no linealidad\n",
    "        \n",
    "        C = np.zeros((n, b, d)) # contenido de la celula\n",
    "        Ct = np.zeros((n, b, d)) # tangente del contenido de la celula\n",
    "        \n",
    "        for t in range(len(n)):\n",
    "            #concatenación de [x,h] como entrada del LSTM\n",
    "            prevh = Hout[t-1] if t > 0 else h0\n",
    "            Hin[t,:,0] = 1 # sesgo\n",
    "            Hin[t,:,1:tam_entrada+1] = X[t]\n",
    "            Hin[t,:,tam_entrada+1:] = prevh\n",
    "            EOSP[t] = Hin[t].dot(WLSTM)\n",
    "            #no linealidades\n",
    "            \n",
    "            EOSPd[t,:,:3*d] = 1.0/(1.0+np.exp(-EOSPd[t,:,:3*d])) # sigmoides; estas son las puertas\n",
    "            EOSPd[t,:,3*d:] = np.tanh(EOSPd[t,:,3*d:]) # tangente\n",
    "            \n",
    "            #activacion de la celula\n",
    "            prevc = C[t-1] if t > 0 else c0\n",
    "            C[t] = EOSPd[t,:,:d] * EOSPd[t,:,3*d:] + EOSPd[t,:,d:2*d] * prevc\n",
    "            Ct[t] = np.tanh(C[t])\n",
    "            Hout[t] = EOSPd[t,:,2*d:3*d] * Ct[t]\n",
    "            \n",
    "        memoria = {}\n",
    "        memoria['WLSTM'] = WLSTM\n",
    "        memoria['Hout'] = Hout\n",
    "        memoria['EOSPd'] = EOSPd\n",
    "        memoria['EOSP'] = EOSP\n",
    "        memoria['C'] = C\n",
    "        memoria['Ct'] = Ct\n",
    "        memoria['Hin'] = Hin\n",
    "        memoria['c0'] = c0\n",
    "        memoria['h0'] = h0\n",
    "        \n",
    "        return Hout, C[t], Hout[t], memoria\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
